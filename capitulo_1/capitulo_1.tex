\chapter{Introduction} 
\doublespacing
\label{cap:ini}
\vspace{-2cm}


Water is essential for all human activities and agriculture is the largest freshwater consumer. Precipitation is a phenomenon highly susceptible to variability and determines its availability \cite{calzadilla2013climate}. Research and apply accurate statistical models to forecast this phenomena has been acknowledged to play a key role for this sector of the human activity \cite{toth2000comparison}. Given the uncertainty and variability that drives its occurrence, it is recognised that is quite difficult to obtain reliable and accurate prediction models that can spatially forecast this element of the hydrological cycle for short periods of time \cite{brath1997role}. It is known that due to its behaviour and complex structure, precipitation is an variable harder to forecast than other climate variables, given the processes involved in its generation and nonlinear behaviour \cite{jha2018evaluation}.

The precipitation forecasting problem is commonly approached in different ways. The use of remote sensing observation with radars and satellite images addresses the issue based on the extrapolation of current weather condition, for very short term forecasting (scale of minutes). Unfortunately the use of radar and satellite images do not provide a satisfactory assessment of rain intensities in larger scales of time, in addition, using this technique in mountainous regions is difficult because of the occurrence of soil shading and the altitude effect \cite{toth2000comparison}. 

One other mean to obtain rainfall forecasting models is by time series analyses techniques. There are different approaches to time series forecasting, specially for climatic proposes.Traditionally forecasting has long been the domain of linear statistics, usual approaches to time series prediction, such as Box-Jenkins \citeyear{box1976time} or ARIMA (autoregressive integrated moving average) method \cite{pankratz1983forecasting}, considers that time series behaves as linear processes. Despite of its easy understanding and applicability it may be totally inappropriate to implement if the ongoing mechanism is subjected to an nonlinear processes \cite{zhang2003time}. 

In meteorology to deal with non linearity, it is generally used numerical weather prediction models (NWP) in applications such as Global Circulation Models (GCM). NWP is an initial-value problem for which initial data are not available in sufficient quantity and with sufficient accuracy, these models abstract some layers of information by discretizing partial differential equations governing large scale atmospheric flow \cite{ghil1981applications}. GCM models are based on highly complex mathematical representations of atmospheric, oceanic, and continental processes being capable to predict climate patterns of different variables such as air temperature, precipitation, atmospheric gases and its behaviour. These models simulates climatic parameters only at grid points requiring downscale of regional models to local models \cite{alotaibi2018future}. 

NWP can active acceptable accuracy in forecasting some meteorological phenomenas but when dealing with rainfall they yet have not active it \cite{ramirez2006linear}, mainly because of the physical complexity of precipitation processes and the reduced temporal and spacial scale involved in such phenomena that numerical models cannot resolve \cite{kuligowski1998localized}. It is required turbulent parameterizations to accurately represent the planetary boundary layer, shallow convection, subgrid-scale cloud cover, and turbulent fluxes related to deep convective systems which are required to future climate projections. Due to limitation on knowledge about cloud-aerosol interactions which are a major source of uncertainties on NWP models leads to far-reaching consequences on the development and accuracy of precipitation models \cite{prein2015review}. One other drawback that NWP models such as GCM have is that they are computationally demanding and require powerful and expensive hardware to be implemented in a meteorological prediction center.  Limitations in computing power may result in inability to appropriately resolve the important climate processes. Low-resolution models fail to capture many important phenomena on regional and lesser scales such as clouds. Downscaling to higher-resolution models introduces boundary interactions that can contaminate the modeling area and propagate error \cite{alotaibi2018future}.

More recently researchers have been approaching such problem with artificial neural networks (ANN) which are a powerful alternative to traditional time-series modelling \cite{zhang1998linear} as for NWP models. ANNs are data-driven self adaptive methods that are able to understand and solve problems of which there is not enough data or observations to use more traditional statistical models \cite{zhang1998forecasting}, rainfall is such a phenomena and ANNs are suited and studied solution.

ANNs are a type of nonlinear model inspired by sophisticated functionalities of human brain. They are universal function approximators that can adaptively discover patterns from data, learn from experience and estimate any complex functional relationship with high accuracy \cite{zhang1998linear, wang2003artificial}, they mimics the brain functionalities both in knowledge acquisition through a learning process and memory by storing synaptic weights as acquired knowledge \cite{ferraudo}. ANNs have an nonparametric nature which enables the development of models without any prior knowlege of the population, its distribution or possible interaction between variables that are commonly used in parametric statistical models \cite{walczak2019artificial}.

ANNs simulates an reduced set of concepts derived from biological neural systems by emulating the electrical activity of the brain of which each part of the neurone plays a role in the communication of the information throughout its parts. Computations and analysis of the brain are achieved by sending electric signals through its processing units which consists of dendrites, axons, terminal buttons and synapses \cite{krogh2008artificial}. Dendrites receives signals from over to the cell body of the neuron. The axon receives signals from cell body and carries them through the sinapses to neighbour neurones dendrites. In math models the processing units, are interconnected in layers or vectors which the output of each neuron serves as input for neighbour neurones. When an electric signal travels from the dendrites to the pre synaptic membrane of the synapse a chemical called neurotransmitter is released in proportional amount to the strength of the signal. The neurotransmitter, diffused within the gap between the synaptic membrane and the neighbour dendrites forces the receiving neurone to generate a new signal that obeys the same set of rules to transmit its impulse \cite{basheer2000artificial}. The amount of signal passed depends on the intensity of the signal emanated from feeding neurons, its strength and the activation threshold of the receiving neuron which can assist or inhibit the firing neurone. This simplified biological mechanism of signal transferring are the bases of ANN and neurocomputing.

The first artificial neurone model was proposed by McCulloch and Pitts in 1943, it was designed to behaviour as a switch which alter its state depending on its input passing through an weight distribution process. The weight multiplies the inputs corresponding to the strength of the synapses that represents the contact between nerve cells \cite{mcculloch1943logical} which can be both positive or excitatory, allowing the electrical pulse to pass, and negative or inhibitory blocking the signals. 

In \citeyear{rosenblatt1958perceptron} \citeauthor{rosenblatt1958perceptron} to understand the process of perceptual recognition of higher organisms and to answer three fundamental questions of neural thinking: How the biological system senses and detects information? What is the form that it is stored and remembered? How storage information influences on recognition behaviour? Proposed an hypothetical nervous system called perceptron. The perceptron was designed to illustrate properties of intelligent system without being deeply attached into unknown meshes which are the natural condition for biological organisms. The machine establishes a mapping between the inputs activity and the output signal by passing signals through a linear threshold function and transmitting its signal to other neurons or the environment. By using weights in the connections the signals can be both excited enhancing its strength or inhibited reducing it \cite{basheer2000artificial}. 

The perceptron weights and thresholds can be adjusted in a training processes. This process is equivalent to approximating the output of the neuron to the corresponding desired counterpart or goal by minimising an error function computed by the difference between the goal of a training set and the output of the ANN in a search for minima \cite{ramchoun2016multilayer}. The perceptron is a single element ANN that responds correctly to as many patterns as possible, being able to respond correctly with high probability to input patterns that were not included in the training set if the output is binary \cite{widrow199030}. In \citeyear{minsky1969perceptrons} \citeauthor{minsky1969perceptrons} mathematically proved the limitations of the perceptron and other types of ANNs when dealing with non linear separable patterns.  

 With the rediscovery of the backpropagation algorithm by \citeauthor{rumelhart1985learning} (\citeyear{rumelhart1985learning}) originally proposed by \citeauthor{Werbos:74} (\citeyear{Werbos:74}) solved the problem of training and implementing non linear solvers that handle non linear groups of variables. To handle non linear problems intermediate layers connected in nodes are added between input and output neurones of the ANN, since this layers of neurons do not connect to the external world they are called hidden layers. This structure is called Multilayer Perceptron (MLP). With the addition of intermediary layers to the perceptron using analogous dynamics and with the implementation of nonlinear training algorithms (backpropagation) the neurons process information and pass over to the output layer with accuracy. 

In the field of agriculture and applied math ANNs has been a successful tool to forecast meteorological indexes. \citeauthor{kumarasiri2006rainfall}  (\citeyear{kumarasiri2006rainfall}) proposed three Neural Network models  based on the feedforward backpropagation architecture to forecast rainfall in a short-term or one day ahead, medium-term or one month ahead and long-term or a year in the city of Colombo in Sri Lanka. The researchers obtaining an accuracy from short to long term of 74.25\%, 58.33\% and 76.67\%. According to the author the region had well defined seasons and long strings of observations with rain in the monsoon seasons and no rain observation days which contributed to the performance of the models. 

To simulate chaotic rainfall events in the suburbs of Sydney, Australia \citeauthor{nasseri2008optimized} (\citeyear{nasseri2008optimized}) proposed an architecture of ANN that is efficient in events with a scale of minutes. The architecture was based on the feedforward backpropagation coupled with genetic algorithm. The genetic algorithm are a kind of computational models inspired in evolution, they encode problem solution on a chromosome-like structure coupled with operators that recombines structures to preserve critical operations and can be viewed as function optimisers \cite{whitley1994genetic}. The authors reported that the study led to conclude that associating ANN with genetic algorithm performed with accuracy and given the high variance and turbulence of precipitation events cumulative data leads to increasing statistical performance and when comparing rainfall forecasting to discrete data types.

Some studies used  ANN models together with NWP models. \citeauthor{ramirez2006linear} (\citeyear{ramirez2006linear}) to generate accurate rainfall forecasts over southeastern Brazil areas used artificial neural networks to downscale the Eta Model with a resolution of 40 x 40 km to forecast variables at a weather station level. The Eta model is a state of the art atmospheric model (NWP model) used for research and operational purposes. The study were able to conclude that ANNs are effective to adjust rainfall forecast for specific points and that NWP models accuracy are reversibly proportional to ANNs in events of heavy rain because they are more effective in events with higher threshold.

The rainfall due to the complexity of the physical processes involved and its variability in space and time is a difficult variable to forecast. Mapping the effect of temporal and spatial information on short term rainfall forecast is a key component into the development of a successful model. Rainfall is considered a Markovian process \cite{luk2000study} that  is a particular case of a stochastic process with discrete estates, which implies that its volume at a given location in a place and time is function of a previous set of observations. Considering this factors knowing the relation between future and past rainfall events is crucial to develop an appropriate ANN architecture that maps this relations and is able to carry over the momentum and accurate to predict. In previous studies, while investigating the effect of temporal and spatial rainfall events in very short periods of time, \citeauthor{luk2000study} (\citeyear{luk2000study}) revealed that there is an optimal limit temporal and spacial limit for inclusion into a ANN. The author also demonstrated that too much or too little spacial information can degrade its performance and that for short term rainfall it might not have long therm memory indicating that with lower lags consistently produced smaller prediction errors. Other authors corroborates with this statement and proved that ANN are able to generalize and use previous input to accurate forecast. \citeauthor{french1992rainfall} (\citeyear{french1992rainfall}) demonstrated that by only increasing the number of training iterations the performance can be improved which is not the case on independent data.


In other studies \cite{kumarasiri2006rainfall, nasseri2008optimized, ramirez2006linear, luk2000study, french1992rainfall, toth2000comparison, partal2015daily} the goal was to numerically predict, with a single ANN structure, the accumulative volume of precipitation in a given scale in a future period of time. The performance of these models were very correlated to the time scale of events that ANNs had to handle. In larger scale of time, such as months, the performance of ANNs are vastly superior then in shorter periods of time. This happens because in larger periods of time the probability of some precipitation be recorded is greater, consecutively models are not biased by a big number of observations with zero precipitation \cite{schoof2001downscaling} and in short scale of time rainfalls are dependent on small scale and unstable physical processes \cite{kuligowski1998localized}.

The objective of this research is to create a methodology to predict the occurrence of rainfall. This is done by constraining the complexity of the predicted events by reducing the variance and rising the bias of the time series. To achieve this objective, a structure of artificial neural networks is being proposed which identifies the signs that lead to the occurrence of rain for each climatic season in short periods of time, letting the ANNs to predict whether or not it is going to rain. The proposed model is intended to filter which days are propitious to rain, so that only the climate variables in the periods that lead to rain are used in quantitative models. With this technique quantitative models can improve its forecasting performance in shorter periods of time and consecutively becoming computationally lighter by reducing the volume of data used in the training stage of the models. 



%
%------
%Dynamic models such as Global circulation models (GCMs) and regional climate models have
%played a significant role in making predicting meteorological and hydrological parameters [6–11].
%The simulations are based on highly complex mathematical representations of atmospheric, oceanic,
%and continental processes. These models can predict future climate patterns for likely future emissions
%scenarios developed by the Intergovernmental Panel on Climate Change (IPCC) [10]. However, GCMs
%can simulate the climatic parameters only at grid points which requires downscaling to regional level
%by regional climate models. Significant uncertainties have been reported in GCM and regional climate
%models results [1,12–18]. Krysanova et al. [1] reported that an average fraction of uncertainty for the
%annual-mean-flow predictions for various basins was up to 57% for GCM and up to 27% for regional
%climate models. Gao et al. [13] concluded that climate change models should always be used cautiously
%because of their uncertainties, due to: (i) the downscaling of broader GCM results to regional levels;
%(ii) uncertainty in the selection of future emissions scenarios; (iii) identification of parameters for
%GCMs; (iv) simplification of processes itself on which the results of GCMs are based; (v) data errors;
%and (vi) size and of dataset [19–22]. The two main types of downscaling techniques including statistical
%and dynamical downscaling have several further categories. Statistical downscaling techniques
%are simple but need fine tuning [23,24]. Dynamical downscaling requires extensive resources to
%establish a high resolution grid to accurately interpolate the global simulations into small-scale regional
%predictions [23–26]. The emissions scenarios are based on expert knowledge and need to be carefully
%selected [24–27].
%
%------