\chapter{Introduction} 
\label{cap:ini}
\vspace{-2cm}


Water is essential for all human activities and agriculture is the largest freshwater consumer. Precipitation a phenomenon highly susceptible to variability, determines its availability\cite{calzadilla2013climate}. Research and apply accurate statistical models to forecast this phenomena has been acknowledged to play a key role for this sector of the human activity\cite{toth2000comparison}. Given the uncertainty and variability that drives its occurrence, it is recognised that is quite difficult to obtain reliable and accurate prediction models that can spatialy forecast this element of the hydrological cycle for short periods of time. \cite{brath1997role}. It is known that due to its behavior and complex structure, precipitation is an variable harder to forecast than other climate variables, given the processes involved in its generation and nonlinear behaviour\cite{jha2018evaluation}.

The precipitation forecasting problem is commonly approached in different ways. The use of remote sensing observation with radars and satellite images addresses the issue based on the extrapolation of current weather condition, for very short term forecasting (scale of minutes). Unfortunately the use of radar and satellite images do not provide a satisfactory assessment of rain intensities in larger scales of time, in addition, using this technique in mountainous regions is difficult because of the occurrence of soil shading and the altitude effect\cite{toth2000comparison}. 

One other mean to obtain rainfall forecasting models is by time series analyses techniques. There are different approaches to time series forecasting, specially for climatic proposes.Traditionally forecasting has long been the domain of linear statistics, usual approaches to time series prediction, such as Box-Jenkins\cite{box1976time} or ARIMA (autoregressive integrated moving average) method\cite{pankratz1983forecasting}, considers that time series behaves as linear processes. Despite of its easy understanding and applicability it may be totally inappropriate to implement if the ongoing mechanism is subjected to an nonlinear processes \cite{zhang2003time}. 

In meteorology to deal with non linearity, it is generally used numerical weather prediction models (NWP) in applications such as Global Circulation Models (GCM). NWP is an initial-value problem for which initial data are not available in sufficient quantity and with sufficient accuracy, these models abstract some layers of information by discretising partial differential equations governing large scale atmospheric flow \cite{ghil1981applications}. GCM models are based on highly complex mathematical representations of atmospheric, oceanic, and continental processes being capable to predict climate patterns of different variables such as temperature, precipitation, atmospheric gases and its behaviour. These models simulates climatic parameters only at grid points requiring downscale of regional models to local models\cite{alotaibi2018future}. 

NWP can active acceptable accuracy in forecasting some meteorological phenomenas but when dealing with rainfall they yet have not active it \cite{ramirez2006linear}, mainly because of the physical complexity of precipitation processes and the reduced temporal and spacial scale involved in such phenomena that numerical models cannot resolve \cite{kuligowski1998localized}. It is required turbulent parameterisations to accurately represent the planetary boundary layer, shallow convection, subgrid-scale cloud cover, and turbulent fluxes related to deep convective systems which are required to future climate projections. Due to limitation on knowledge about cloud-aerosol interactions which are a major source of uncertainties on NWP models leads to far-reaching consequences on the development and accuracy of precipitation models \cite{prein2015review}. One other drawback that NWP models such as GCM have is that they are computationally demanding and require powerful and expensive hardware to be implemented in a meteorological prediction center.  Limitations in computing power may result in inability to appropriately resolve the important climate processes. Low-resolution models fail to capture many important phenomena on regional and lesser scales such as clouds. Downscaling to higher-resolution models introduces boundary interactions that can contaminate the modeling area and propagate error \cite{alotaibi2018future}

More recently researchers have been approaching such problem with artificial neural networks(ANN) which are a powerful alternative to traditional time-series modelling \cite{zhang1998linear} as for NWP models. ANNs are data-driven self adaptive methods that are able to understand and solve problems of which there's not enough data or observations to use more traditional statistical models\cite{zhang1998forecasting}, rainfall is such a phenomena and ANNs are suited and studied solution.

ANNs are a type of nonlinear model inspired by sophisticated functionalities of human brain. They are universal function approximators that can adaptively discover patterns from data, learn from experience and estimate any complex functional relationship with high accuracy\cite{zhang1998linear, wang2003artificial}, they mimics the brain functionalities both in knowledge acquisition through a learning process and memory by storing synaptic weights as acquired knowledge\cite{ferraudo}. ANNs have an nonparametric nature which enables the development of models without any prior knowlege of the population, its distribution or possible interaction between variables that are commonly used in parametric statistical models.\cite{walczak2019artificial}

ANNs simulates an reduced set of concepts derived from biological neural systems by emulating the electrical activity of the brain of which each part of the neurone plays a role in the communication of the information throughout its parts. Computations and analysis of the brain are achieved by sending electric signals through its processing units which consists of dendrites, axons, terminal buttons and synapses\cite{krogh2008artificial}. Dendrites receives signals from over to the cell body of the neuron. The axon receives signals from cell body and carries them through the sinapses to neighbour neurones dendrites. In math models the processing units, are interconnected in layers or vectors which the output of each neuron serves as input for neighbour neurones. When an electric signal travels from the dendrites to the pre synaptic membrane of the synapse a chemical called neurotransmitter is released in proportional amount to the strength of the signal. The neurotransmitter, diffused within the gap between the synaptic membrane and the neighbour dendrites forces the receiving neurone to generate a new signal that obeys the same set of rules to transmit its impulse\cite{basheer2000artificial}. The amount of signal passed depends on the intensity of the signal emanated from feeding neurons, its strength and the activation threshold of the receiving neuron which can assist or inhibit the firing neurone. This simplified biological mechanism of signal transferring are the bases of ANN and neurocomputing.

The first artificial neurone model was proposed by McCulloch and Pitts in 1943, it was designed to behaviour as a switch which alter its state depending on its input passing through an weight distribution process. The weight multiplies the inputs corresponding to the strength of the synapses that represents the contact between nerve cells \cite{mcculloch1943logical} which can be both positive or excitatory, allowing the electrical pulse to pass, and negative or inhibitory blocking the signals. in \citeyear{rosenblatt1958perceptron} \citeauthor{rosenblatt1958perceptron} to understand the process of perceptual recognition of higher organisms and to answer three fundamental questions of neural thinking: How the biological system senses and detects information? What is the form that it is stored and remembered? How storage information influences on recognition behaviour? Proposed an hypothetical nervous system called perceptron. The perceptron was designed to illustrate properties of intelligent system without being deeply attached into unknown meshes which are the natural condition for biological organisms. The machine establishes a mapping between the inputs activity and the output signal by passing signals through a linear threshold function and transmitting its signal to other neurons or the environment. By using weights in the connections the signals can be both excited enhancing its strength or inhibited reducing it \cite{basheer2000artificial}. 


%In the 1960s, it was shown that networks of such
%model neurons have properties similar to the
%brain: they can perform sophisticated pat
%tern recognition, and they can function even
%if some of the neurons are destroyed. The
%demonstration, in particular by Rosenblatt,
%that simple networks of such model neurons
%called ‘perceptrons’ could learn from
%examples stimulated interest in the field,
%but after Minsky and Papert1 showed that
%simple perceptrons could solve only the very
%limited class of linearly separable problems
%(see below), activity in the field diminished.
%Nonetheless, the error back-propagation
%method2, which can make fairly complex networks
%of simple neurons learn from examples,
%showed that these networks could solve
%problems that were not linearly separable.
%NETtalk, an application of an artificial neural
%network for machine reading of text, was
%one of the first widely known applications3,
%and many followed soon after. In the field of
%biology, the exact same type of network as
%in NETtalk was also applied to prediction of
%protein secondary structure4; in fact, some
%of the best predictors still use essentially the
%same method. Another big wave of interest
%in artificial neural networks started, and led
%to a fair deal of hype about magical learning
%and thinking machines. Some of the important
%early works are gathered in ref. 5.

In the field of agriculture and applied math ANNs has been a successful tool to forecast meteorological indexes  \cite{kumarasiri2006rainfall, nasseri2008optimized, ramirez2006linear, luk2000study, french1992rainfall, toth2000comparison, partal2015daily}. In these studies the goal was to numerically predict, with a single ANN structure, the accumulative volume of precipitation in a given scale in a future period of time. The performance of these models were very correlated to the time scale of events that ANNs had to handle. In larger scale of time, such as months, the performance of ANNs are vastly superior then in shorter periods of time. This happens because in larger periods of time the probability of some precipitation be recorded is greater, consecutively models are not biased by a big number of observations with zero precipitation \cite{schoof2001downscaling} and in short scale of time rainfalls are dependent on small scale and unstable physical processes \cite{kuligowski1998localized}.

The objective of this paper is to create a methodology to predict the occurrence of rainfall. This is done by constraining the complexity of the predicted events by reducing the variance and rising the bias of the time series. To achieve this objective, a structure of artificial neural networks is being proposed which identifies the signs that lead to the occurrence of rain for each climatic season in short periods of time, letting the ANNs to predict whether or not it is going to rain. The proposed model is intended to filter which days are propitious to rain, so that only the climate variables in the periods that lead to rain are used in quantitative models. With this technique quantitative models can improve its forecasting performance in shorter periods of time and consecutively becoming computationally lighter by reducing the volume of data used in the training stage of the models. 



%
%------
%Dynamic models such as Global circulation models (GCMs) and regional climate models have
%played a significant role in making predicting meteorological and hydrological parameters [6–11].
%The simulations are based on highly complex mathematical representations of atmospheric, oceanic,
%and continental processes. These models can predict future climate patterns for likely future emissions
%scenarios developed by the Intergovernmental Panel on Climate Change (IPCC) [10]. However, GCMs
%can simulate the climatic parameters only at grid points which requires downscaling to regional level
%by regional climate models. Significant uncertainties have been reported in GCM and regional climate
%models results [1,12–18]. Krysanova et al. [1] reported that an average fraction of uncertainty for the
%annual-mean-flow predictions for various basins was up to 57% for GCM and up to 27% for regional
%climate models. Gao et al. [13] concluded that climate change models should always be used cautiously
%because of their uncertainties, due to: (i) the downscaling of broader GCM results to regional levels;
%(ii) uncertainty in the selection of future emissions scenarios; (iii) identification of parameters for
%GCMs; (iv) simplification of processes itself on which the results of GCMs are based; (v) data errors;
%and (vi) size and of dataset [19–22]. The two main types of downscaling techniques including statistical
%and dynamical downscaling have several further categories. Statistical downscaling techniques
%are simple but need fine tuning [23,24]. Dynamical downscaling requires extensive resources to
%establish a high resolution grid to accurately interpolate the global simulations into small-scale regional
%predictions [23–26]. The emissions scenarios are based on expert knowledge and need to be carefully
%selected [24–27].
%
%------